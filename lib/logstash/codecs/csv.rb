# encoding: utf-8
require "logstash/codecs/base"
require "logstash/util/charset"
require "logstash/util/buftok"
require "csv"

class LogStash::Codecs::CSV < LogStash::Codecs::Base

  config_name "csv"

  # When decoding:
  #   Define a list of column names (in the order they appear in the CSV,
  #   as if it were a header line). If `columns` is not configured, or there
  #   are not enough columns specified, the default column names are
  #   "column1", "column2", etc.
  # When encoding:
  #   List of fields names to include in the encoded CSV, in the order listed.
  config :columns, :validate => :array, :default => []

  # Define the column separator value. If this is not specified, the default
  # is a comma `,`.
  # Optional.
  config :separator, :validate => :string, :default => ","

  # Define the character used to quote CSV fields. If this is not specified
  # the default is a double quote `"`.
  # Optional.
  config :quote_char, :validate => :string, :default => '"'

  # When encoding in an output plugin, include headers in the encoded CSV
  # once per codec lifecyle (not for every event). Default => false
  config :include_headers, :validate => :boolean, :default => false

  # Define whether column names should be autogenerated or not.
  # Defaults to true. If set to false, columns not having a header specified will not be parsed.
  config :autogenerate_column_names, :validate => :boolean, :default => true

  # Define whether empty columns should be skipped.
  # Defaults to false. If set to true, columns containing no value will not be included.
  config :skip_empty_columns, :validate => :boolean, :default => false

  # Define whether column names should be auto-detected from the header column or not.
  # Defaults to false.
  config :autodetect_column_names, :validate => :boolean, :default => false

  # Define the line delimiter
  config :delimiter, :validate => :string, :default => "\n"

  # Define a set of datatype conversions to be applied to columns.
  # Possible conversions are integer, float, date, date_time, boolean
  #
  # # Example:
  # [source,ruby]
  #     filter {
  #       csv {
  #         convert => { "column1" => "integer", "column2" => "boolean" }
  #       }
  #     }
  config :convert, :validate => :hash, :default => {}

  # The character encoding used in this codec. Examples include "UTF-8" and
  # "CP1252".
  config :charset, :validate => ::Encoding.name_list, :default => "UTF-8"

  CONVERTERS = {
    :integer => lambda do |value|
      CSV::Converters[:integer].call(value)
    end,

    :float => lambda do |value|
      CSV::Converters[:float].call(value)
    end,

    :date => lambda do |value|
      result = CSV::Converters[:date].call(value)
      result.is_a?(Date) ? LogStash::Timestamp.new(result.to_time) : result
    end,

    :date_time => lambda do |value|
      result = CSV::Converters[:date_time].call(value)
      result.is_a?(DateTime) ? LogStash::Timestamp.new(result.to_time) : result
    end,

    :boolean => lambda do |value|
       value = value.strip.downcase
       return false if value == "false"
       return true  if value == "true"
       return value
    end
  }
  CONVERTERS.default = lambda {|v| v}
  CONVERTERS.freeze

  def register
    @buffer = FileWatch::BufferedTokenizer.new(@delimiter)
    @converter = LogStash::Util::Charset.new(@charset)
    @converter.logger = @logger

    # validate conversion types to be the valid ones.
    bad_types = @convert.values.select do |type|
      !CONVERTERS.has_key?(type.to_sym)
    end.uniq
    raise(LogStash::ConfigurationError, "Invalid conversion types: #{bad_types.join(', ')}") unless bad_types.empty?

    # @convert_symbols contains the symbolized types to avoid symbol conversion in the transform method
    @convert_symbols = @convert.each_with_object({}){|(k, v), result| result[k] = v.to_sym}

    # if the zero byte character is entered in the config, set the value
    if (@quote_char == "\\x00")
      @quote_char = "\x00"
    end

    @logger.debug? && @logger.debug("CSV parsing options", :col_sep => @separator, :quote_char => @quote_char)
  end

  def decode(data, &block)
    puts("*** data=#{data.inspect}")
    @buffer.extract(data).each do |line|
      puts("*** line=#{line.inspect}")
      parse(@converter.convert(line), &block)
    end
  end

  def encode(event)
    if @include_headers
      csv_data = CSV.generate_line(select_keys(event), :col_sep => @separator, :quote_char => @quote_char, :headers => true)
      @on_event.call(event, csv_data)

      # output headers only once per codec lifecycle
      @include_headers = false
    end

    csv_data = CSV.generate_line(select_values(event), :col_sep => @separator, :quote_char => @quote_char)
    @on_event.call(event, csv_data)
  end

  def flush(&block)
    remainder = @buffer.flush
    if !remainder.empty?
      puts("flush: remainder=#{remainder}")
      parse(@converter.convert(remainder), &block)
    end
  end

  private

  def parse(line, &block)
    begin
      values = CSV.parse_line(line, :col_sep => @separator, :quote_char => @quote_char)
      return if values.nil?

      if (@autodetect_column_names && @columns.empty?)
        @columns = values
        @logger.debug? && @logger.debug("Auto detected the following columns", :columns => @columns.inspect)
        return
      end

      decoded = {}
      values.each_index do |i|
        unless (@skip_empty_columns && (values[i].nil? || values[i].empty?))
          unless ignore_field?(i)
            field_name = @columns[i] || "column#{i + 1}"
            decoded[field_name] = transform(field_name, values[i])
          end
        end
      end

      yield LogStash::Event.new(decoded)
    rescue CSV::MalformedCSVError => e
      @logger.error("CSV parse failure. Falling back to plain-text", :error => e, :data => line)
      yield LogStash::Event.new("message" => line, "tags" => ["_csvparsefailure"])
    end
  end

  def select_values(event)
    if @columns.empty?
      event.to_hash.values
    else
      @columns.map {|column| event.get(column)}
    end
  end

  def select_keys(event)
    @columns.empty? ? event.to_hash.keys : @columns
  end

  def ignore_field?(index)
    !@columns[index] && !@autogenerate_column_names
  end

  def transform(field_name, value)
    CONVERTERS[@convert_symbols[field_name]].call(value)
  end
end
